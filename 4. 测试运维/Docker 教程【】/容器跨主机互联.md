# docker 多种跨主机访问选择哪一种

**网上共搜罗到五种方案：**

**一、利用OpenVSwitch**

**二、利用Weave**

**三、Docker在1.9之后支持的Overlay network（官方的做法）Docker 1.9 Overlay Network实现跨主机网络互通**

**四、将多个物理机的容器组到一个物理网络来1.创建自己的网桥br02.将docker默认网桥绑定到br0多台物理主机之间的容器互联**

**五、修改主机docker默认的虚拟网段，然后在各自主机上分别把对方的docker网段加入到路由表中，配合iptables即可实现docker容器跨主机通信**

 

 ### 理解Docker跨多主机容器网络

在[Docker 1.9](https://blog.docker.com/tag/docker-1-9/) 出世前，跨多主机的容器通信方案大致有如下三种：

1、[端口映射](http://tonybai.com/2016/01/18/understanding-binding-docker-container-ports-to-host/)

将宿主机A的端口P映射到容器C的网络空间监听的端口P’上，仅提供四层及以上应用和服务使用。这样其他主机上的容器通过访问宿主机A的端口P实 现与容器C的通信。显然这个方案的应用场景很有局限。

2、将物理网卡桥接到虚拟网桥，使得容器与宿主机配置在同一网段下

在各个宿主机上都建立一个新虚拟网桥设备br0，将各自物理网卡eth0桥接br0上，eth0的IP地址赋给br0；同时修改[Docker](http://lib.csdn.net/base/docker) daemon的DOCKER_OPTS，设置-b=br0（替代docker0），并限制[Container](http://lib.csdn.net/base/docker) IP地址的分配范围为同物理段地址（–fixed-cidr）。重启各个主机的Docker Daemon后，处于与宿主机在同一网段的Docker容器就可以实现跨主机访问了。这个方案同样存在局限和扩展性差的问题：比如需将物理网段的地址划分 成小块，分布到各个主机上，防止IP冲突；子网划分依赖物理交换机设置；Docker容器的主机地址空间大小依赖物理网络划分等。

3、使用第三方的基于[SDN](https://en.wikipedia.org/wiki/Software-defined_networking)的方案：比如 使用[Open vSwitch – OVS](http://openvswitch.org/) 或[CoreOS](https://coreos.com/)的[Flannel](https://github.com/coreos/flannel) 等。

关于这些第三方方案的细节大家可以参考O’Reilly的《[Docker Cookbook](http://book.douban.com/subject/26631435/)》 一书。

Docker在1.9版本中给大家带来了一种原生的跨多主机容器网络的解决方案，该方案的实质是采用了基于[VXLAN](https://datatracker.ietf.org/doc/rfc7348) 的覆盖网技术。方案的使用有一些前提条件：

1、[Linux](http://lib.csdn.net/base/linux) Kernel版本 >= 3.16；
2、需要一个外部Key-value Store（官方例子中使用的是[consul](https://en.wikipedia.org/wiki/Software-defined_networking)）；
3、各物理主机上的Docker Daemon需要一些特定的启动参数；
4、物理主机允许某些特定TCP/UDP端口可用。



--------

## Docker容器跨主机通信之：直接路由方式



[codesheep](https://yq.aliyun.com/users/rd4tb24sejyqy)  

- [docker](https://yq.aliyun.com/tags/type_blog-tagid_72/)
-  

- [配置](https://yq.aliyun.com/tags/type_blog-tagid_698/)
-  

- [主机](https://yq.aliyun.com/tags/type_blog-tagid_964/)
-  

- [容器](https://yq.aliyun.com/tags/type_blog-tagid_985/)
-  

- [centos](https://yq.aliyun.com/tags/type_blog-tagid_1061/)
-  

- [专有云](https://yq.aliyun.com/tags/type_blog-tagid_1790/)


## 概述

就目前Docker自身默认的网络来说，单台主机上的不同Docker容器可以借助docker0网桥直接通信，这没毛病，而不同主机上的Docker容器之间只能通过在主机上用映射端口的方法来进行通信，有时这种方式会很不方便，甚至达不到我们的要求，因此位于不同物理机上的Docker容器之间直接使用本身的IP地址进行通信很有必要。再者说，如果将Docker容器起在不同的物理主机上，我们不可避免的会遭遇到Docker容器的跨主机通信问题。本文就来尝试一下。



## 情景构造

如下图所示，我们有两个物理主机1和主机2，我们在各自宿主机上启动一个centos容器，启动成功之后，两个容器分别运行在两个宿主机之上，默认的IP地址分配如图所示，这也是Docker自身默认的网络。

![两台主机上的容器如何通信？](https://upload-images.jianshu.io/upload_images/9824247-88cc8412a6ca401b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

此时两台主机上的Docker容器如何直接通过IP地址进行通信？

一种直接想到的方案便是通过分别在各自主机中 **添加路由** 来实现两个centos容器之间的直接通信。我们来试试吧



## 方案原理分析

由于使用容器的IP进行路由，就需要避免不同主机上的容器使用了相同的IP，为此我们应该为不同的主机分配不同的子网来保证。于是我们构造一下两个容器之间通信的路由方案，如下图所示。

![容器间通信](https://upload-images.jianshu.io/upload_images/9824247-60260e72bd7d7739.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**各项配置如下：**

- 主机1的IP地址为：192.168.145.128
- 主机2的IP地址为：192.168.145.129
- 为主机1上的Docker容器分配的子网：172.17.1.0/24
- 为主机2上的Docker容器分配的子网：172.17.2.0/24

这样配置之后，两个主机上的Docker容器就肯定不会使用相同的IP地址从而避免了IP冲突。

我们接下来 **定义两条路由规则** 即可：

- 所有目的地址为172.17.1.0/24的包都被转发到主机1上
- 所有目的地址为172.17.2.0/24的包都被转发到主机2上

**综上所述，数据包在两个容器间的传递过程如下：**

- 从container1 发往 container2 的数据包，首先发往container1的“网关”docker0，然后通过查找主机1的路由得知需要将数据包发给主机2，数据包到达主机2后再转发给主机2的docker0，最后由其将数据包转到container2中；反向原理相同，不再赘述。

我们心里方案想的是这样，接下来实践一下看看是否可行。


## 实际试验

- **0x01. 分别对主机1和主机2上的docker0进行配置**

编辑主机1上的 `/etc/docker/daemon.json` 文件，添加内容：`"bip" : "ip/netmask"`

```
{ "bip", "172.17.1.252/24" }
```

编辑主机2上的 `/etc/docker/daemon.json` 文件，添加内容：`"bip" : "ip/netmask"`

```
{ "bip", "172.17.2.252/24" }
```

- **0x02. 重启docker服务**

主机1和主机2上均执行如下命令重启docker服务以使修改后的docker0网段生效

```
systemctl restart docker
```

- **0x03. 添加路由规则**

主机1上添加路由规则如下：

```
route add -net 172.17.2.0 netmask 255.255.255.0 gw 192.168.145.129
```

主机2上添加路由规则如下：

```
route add -net 172.17.1.0 netmask 255.255.255.0 gw 192.168.145.128
```

- **0x04. 配置iptables规则**

主机1上添加如下规则：

```
iptables -t nat -F POSTROUTING
iptables -t nat -A POSTROUTING -s 172.17.1.0/24 ! -d 172.17.0.0/16 -j MASQUERADE
```

主机2上添加如下规则：

```
iptables -t nat -F POSTROUTING
iptables -t nat -A POSTROUTING -s 172.17.2.0/24 ! -d 172.17.0.0/16 -j MASQUERADE
```

- **0x05. 启动容器**

主机1上启动centos容器：

```
docker run -it --name container1 centos /bin/bash
```

主机2上启动centos容器：

```
docker run -it --name container2 centos /bin/bash
```

- **0x06. 容器间直接通信**

好了，现在两容器可以互ping了

![container1 ping container2](https://upload-images.jianshu.io/upload_images/9824247-26c74d0e5c60c372.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![container2 ping container1](https://upload-images.jianshu.io/upload_images/9824247-ab84d1af0f0cd2ed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


## 后记

本文探讨了局域网中不同宿主机间Docker容器直接通信的一种可能方案。当然现在实现跨主机容器间通信的现成方案也很多，典型的比如flannel这种，我的 [个人私有云](https://yq.aliyun.com/go/articleRenderRedirect?url=https://www.jianshu.com/p/7d1fb03b8925) 也用的是这种方案。

- [作者更多的原创文章在此](https://yq.aliyun.com/go/articleRenderRedirect?url=http://www.codesheep.cn)

如果有兴趣，也可以抽点时间看看作者一些关于容器化、微服务化方面的文章：

- [Docker容器可视化监控中心搭建](https://yq.aliyun.com/go/articleRenderRedirect?url=https://www.jianshu.com/p/9e47ffaf5e31)
- [利用ELK搭建Docker容器化应用日志中心](https://yq.aliyun.com/go/articleRenderRedirect?url=https://www.jianshu.com/p/a40c36beee63)
- [利用K8S技术栈打造个人私有云系列连载文章](https://yq.aliyun.com/go/articleRenderRedirect?url=https://www.jianshu.com/p/9bc87b5380e8)
- [Spring Boot应用监控实战](https://yq.aliyun.com/go/articleRenderRedirect?url=https://www.jianshu.com/p/e9ce05b44150)
- [RPC框架实践之：Google gRPC](https://yq.aliyun.com/go/articleRenderRedirect?url=https://www.jianshu.com/p/c61fcf2a009f)
- [RPC框架实践之：Apache Thrift](https://yq.aliyun.com/go/articleRenderRedirect?url=https://www.jianshu.com/p/52fa63b222ac)
- [微服务调用链追踪中心搭建](https://yq.aliyun.com/go/articleRenderRedirect?url=https://www.jianshu.com/p/da80ea881424)

------------

# Docker容器跨主机通信之：直接路由方式



## 概述

就目前Docker自身默认的网络来说，单台主机上的不同Docker容器可以借助docker0网桥直接通信，这没毛病，而不同主机上的Docker容器之间只能通过在主机上用映射端口的方法来进行通信，有时这种方式会很不方便，甚至达不到我们的要求，因此位于不同物理机上的Docker容器之间直接使用本身的IP地址进行通信很有必要。再者说，如果将Docker容器起在不同的物理主机上，我们不可避免的会遭遇到Docker容器的跨主机通信问题。本文就来尝试一下。

> **注：** 本文原载于 [**My Personal Blog：**](http://www.codesheep.cn/)， [**CodeSheep · 程序羊**](http://www.codesheep.cn/) ！


## 情景构造

如下图所示，我们有两个物理主机1和主机2，我们在各自宿主机上启动一个centos容器，启动成功之后，两个容器分别运行在两个宿主机之上，默认的IP地址分配如图所示，这也是Docker自身默认的网络。



![img](https://upload-images.jianshu.io/upload_images/9824247-88cc8412a6ca401b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

两台主机上的容器如何通信？

此时两台主机上的Docker容器如何直接通过IP地址进行通信？

一种直接想到的方案便是通过分别在各自主机中 **添加路由** 来实现两个centos容器之间的直接通信。我们来试试吧


## 方案原理分析

由于使用容器的IP进行路由，就需要避免不同主机上的容器使用了相同的IP，为此我们应该为不同的主机分配不同的子网来保证。于是我们构造一下两个容器之间通信的路由方案，如下图所示。



![img](https://upload-images.jianshu.io/upload_images/9824247-60260e72bd7d7739.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

容器间通信

**各项配置如下：**

- 主机1的IP地址为：192.168.145.128
- 主机2的IP地址为：192.168.145.129
- 为主机1上的Docker容器分配的子网：172.17.1.0/24
- 为主机2上的Docker容器分配的子网：172.17.2.0/24

这样配置之后，两个主机上的Docker容器就肯定不会使用相同的IP地址从而避免了IP冲突。

我们接下来 **定义两条路由规则** 即可：

- 所有目的地址为172.17.1.0/24的包都被转发到主机1上
- 所有目的地址为172.17.2.0/24的包都被转发到主机2上

**综上所述，数据包在两个容器间的传递过程如下：**

- 从container1 发往 container2 的数据包，首先发往container1的“网关”docker0，然后通过查找主机1的路由得知需要将数据包发给主机2，数据包到达主机2后再转发给主机2的docker0，最后由其将数据包转到container2中；反向原理相同，不再赘述。

我们心里方案想的是这样，接下来实践一下看看是否可行。



## 实际试验

- **0x01. 分别对主机1和主机2上的docker0进行配置**

编辑主机1上的 `/etc/docker/daemon.json` 文件，添加内容：`"bip" : "ip/netmask"`

```
{ "bip", "172.17.1.252/24" }
```

编辑主机2上的 `/etc/docker/daemon.json` 文件，添加内容：`"bip" : "ip/netmask"`

```
{ "bip", "172.17.2.252/24" }
```

- **0x02. 重启docker服务**

主机1和主机2上均执行如下命令重启docker服务以使修改后的docker0网段生效

```
systemctl restart docker
```

- **0x03. 添加路由规则**

主机1上添加路由规则如下：

```
route add -net 172.17.2.0 netmask 255.255.255.0 gw 192.168.145.129
```

主机2上添加路由规则如下：

```
route add -net 172.17.1.0 netmask 255.255.255.0 gw 192.168.145.128
```

- **0x04. 配置iptables规则**

主机1上添加如下规则：

```
iptables -t nat -F POSTROUTING
iptables -t nat -A POSTROUTING -s 172.17.1.0/24 ! -d 172.17.0.0/16 -j MASQUERADE
```

主机2上添加如下规则：

```
iptables -t nat -F POSTROUTING
iptables -t nat -A POSTROUTING -s 172.17.2.0/24 ! -d 172.17.0.0/16 -j MASQUERADE
```

- **0x05. 启动容器**

主机1上启动centos容器：

```
docker run -it --name container1 centos /bin/bash
```

主机2上启动centos容器：

```
docker run -it --name container2 centos /bin/bash
```

- **0x06. 容器间直接通信**

好了，现在两容器可以互ping了



![img](https://upload-images.jianshu.io/upload_images/9824247-26c74d0e5c60c372.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

container1 ping container2



![img](https://upload-images.jianshu.io/upload_images/9824247-ab84d1af0f0cd2ed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

container2 ping container1


## 后记

> 由于能力有限，若有错误或者不当之处，还请大家批评指正，一起学习交流！

------------

# **OpenvSwitch实现Docker容器跨宿主机互联**

2019.03.09 20:25 113浏览





\1. OpenvSwitch简介

​    Open vSwitch（下面简称为 OVS）是由 Nicira Networks 主导的，运行在虚拟化平台（例如 KVM，Xen）上的虚拟交换机。在虚拟化平台上，OVS可以为动态变化的端点提供 2 层交换功能，很好的控制虚拟网络中的访问策略、网络隔离、流量监控等等。主要实现代码为可移植的C代码。

​    它的目的是让大规模网络自动化可以通过编程扩展，同时仍然支持标准的管理接口和协议（例如NetFlow,sFlow,SPAN,RSPAN,CLI,LACP,802.1ag）。此外，它被设计为支持跨越多个物理服务器的分布式环境，类似于vmware的vmnetwork分布式switch或cisconexus 1000v。Open vSwitch支持多种Linux虚拟化协议，包括Xen/Xen Server,KVM,和VirtualBox。

\2. 通过VxLAN方式实现容器跨宿主机通信

​    本次测试通过OpenvSwitch的VxLAN网络，实现两台物理主机容器能够跨宿主机互访。

2.1 拓扑图

wKiom1a0UMGTMqwEAACeUX6KmvI583.png

2.2 配置ovs

 （1）这里通过两个脚本来配置host10和host11两台主机的OpenvSwitch部分。如下:

\#host10

[root@host10 ~]# cat vsctl-add.sh

\#!/bin/bash

ovs-vsctl add-br br0  #新建两个虚拟交换机

ovs-vsctl add-br br1

ifconfig eth0 0 up    #将物理主机ip赋值给br1

ifconfig br1 192.168.1.10/24 up

route add default gw 192.168.1.1

ovs-vsctl add-port br1 eth0  #将eth0加入br1

ovs-vsctl add-port br0 docker0  #将docker0加入br0

ifconfig br0 172.17.0.2/24 up  #配置br0和docker0的IP

ifconfig docker0 172.17.0.1/24 up

\#host11

[root@host11 ~]# cat vsctl-add.sh

\#!/bin/bash

ovs-vsctl add-br br0  

ovs-vsctl add-br br1

ifconfig eth0 0 up

ifconfig br1 192.168.1.11/24 up

route add default gw 192.168.1.1

ovs-vsctl add-port br1 eth0

ovs-vsctl add-port br0 docker0

ifconfig br0 172.17.0.4/24 up

ifconfig docker0 172.17.0.3/24 up

提示：以上两个脚本在物理主机上通过ssh执行时通过 nohup ./vsctl-add.sh & 方式执行，否则会出现网络断掉执行不成功的情况。

（2）配置VxLAN实现跨主机互联

\#host10

ovs-vsctl add-port br0 vx1 -- set interface vx1 type=vxlan options:remote_ip=192.168.1.11

\#host11

ovs-vsctl add-port br0 vx1 -- set interface vx1 type=vxlan options:remote_ip=192.168.1.10

\#执行完毕后查看

[root@host10 ~]# ovs-vsctl show

a8251e22-bb31-4ee6-8321-49fbd0f1b735

​    Bridge "br0"

​        Port "vx1"

​            Interface "vx1"

​                type: vxlan

​                options: {remote_ip="192.168.1.11"}

​        Port "veth1pl5407"

​            Interface "veth1pl5407"

​        Port "br0"

​            Interface "br0"

​                type: internal

​        Port "docker0"

​            Interface "docker0"

​        Port "veth1pl4977"

​            Interface "veth1pl4977"

​    Bridge "br1"

​        Port "eth0"

​            Interface "eth0"

​        Port "br1"

​            Interface "br1"

​                type: internal

2.3 创建四个容器

​     这里通过pipework固定容器IP地址，以后加入到/etc/rc.local里面实现开机启动自动配置。

\#host10

docker run -itd --net=none --name test1 centos:6 /bin/bash

docker run -itd --net=none --name test2 centos:6 /bin/bash

pipework br0 test1 172.17.0.101/24@172.17.0.1

pipework br0 test2 172.17.0.102/24@172.17.0.1

\#host11

docker run -itd --net=none --name test3 centos:6 /bin/bash

docker run -itd --net=none --name test4 centos:6 /bin/bash

pipework br0 test3 172.17.0.103/24@172.17.0.3

pipework br0 test4 172.17.0.104/24@172.17.0.3

2.4 测试

​     从test1容器访问另外三个容器可以正常通讯。

wKiom1a0WNGyPuTwAABYzUnlqr4810.png

\3. 总结

​    通过本次测试，可以看到OpenvSwitch的强大之处，ovs不仅仅用于docker容器，还可以应用虚拟主机网络。通过软件定义网络，极大的简化了网络配置。docker在1.9版本之后加入overlay网络，运用的也是ovs技术。

©著作权归作者所有：来自51CTO博客作者OrangeBrain的原创作品，如需转载，请注明出处，否则将追究法律责任

网络容器OpenvDocker

-------------

# 解决Flannel跨主机互联网络问题【Docker】



当您将多台服务器节点组成一个Docker集群时，需要对集群网络进行设置，否则默认情况下，无法跨主机容器互联，接下来我们首先分析一下原因。

# **跨主机容器互联**

下图描述了一个简单的集群网络，在该集群内，有两台服务器甲和乙，每台服务器上都有两张网卡，分别连接公网和私网，两台服务器可以通过私网互联，在两个服务器节点上分别安装了Docker，并且运行了A/B/C/D 4个容器。

每台服务器节点上都有一个 docker0 网桥，这是docker启动后初始化的虚拟设备，每个容器都与docker0网桥连接，并且，容器的IP由docker自动分配。



![img](https://upload-images.jianshu.io/upload_images/9873199-ec7df690fbff95f3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

但是这个默认情况下的网络设置不支持跨主机的容器互联，原因有两方面。

**一，跨主机访问容器，没有有效路由**

比如，容器A要访问容器D，请求的地址为 192.168.1.4 ，但是主机甲并不知道该将这个IP发送到那个网络设备上，主机甲也不知道主机乙内部有个容器D。

**二，多个节点上的容器网段冲突**

默认情况下，docker启动后初始化 docker0 网桥时，会随机分配一个IP段，那么，如果不加以协调，多个节点内的容器网络有可能会冲突，比如上图中两个网络都采用了 192.168.1.1/24 网段，在这种情况下，就会导致容器IP冲突，比如 B 和 C。

那么，只需要解决这两个问题，我们就可以实现跨主机的容器互联。

# **脉冲云集群网络设置**

使用脉冲云可以非常轻易地完成集群网络设置。在增加集群时，只需要将集群的网络类型设置为Flannel即可。



![img](https://upload-images.jianshu.io/upload_images/9873199-f56848a2c15cf3d8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/479/format/webp)

Flannel 是一个专门用于容器网络互联的软件，脉冲云会自动地在您的服务器节点上部署Flannel实现容器互联。

设置Flannel时，可以指定容器局域网段和子网掩码，如上图所示，如果选择局域网段为 172.16.0.0/12 子网掩码为 255.255.240.0 那么，在整个集群网络中，就可以分配256个子网，IP段分别为172.16.0.0/20、 172.16.16.0/20、 172.16.32.0/20 等等，每个子网中可以再分配 4096 个IP。每个节点的 docker0 网桥使用一个子网，每个容器使用一个子网内的IP，那么我们就可以组成下图中所示网络。



![img](https://upload-images.jianshu.io/upload_images/9873199-7f0bd04e689ba8bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

图中，主机甲的docker被分配到了 172.16.0.1/20 子网，主机已的docker被分配到了172.16.16.1/20 子网，两个子网都处在一个由Flannel管理的虚拟网络 172.16.0.0/12 中，图中以虚线代表。

到此，在Flannel的协调下，各个主机上的Docker子网IP就不会再冲突了，另外，Flannel会维护容器网络的路由规则，容器A就可以通过172.16.16.3访问容器D了，也就实现了跨主机容器互联。

Flannel维护的容器网络是一个虚拟网络，在图中的虚线也是为了抽象理解，如果你对Flannel的实现方式感兴趣，可以继续查阅Flannel的官方文档。

> 一些说明
>
> 上文中为了简化方便理解，网桥IP和子网IP段没有分开说明，在上图中，主机甲所分配的子网网段是 172.16.0.0/20 ，网段中的第一个IP 172.16.0.1 ，用作网桥设备的IP。
>
> 由于一个网段中第一个IP用作网桥设备IP，最后一个IP用作广播IP，所以在一个子网中，理论上可以分配 4096 个IP，但是实际上只有 4094 个IP可用。
>
> 在设置脉冲云集群网络时，选择的集群网段请勿与已经存在的网络冲突，比如目标集群已经存在了 10.0.0.0/8 网络，那么请选择 172.16.0.0/12 或 192.168.0.0/16 作为容器网络。

# **组网IP**

在上文Flannel网络的示意图中，有三个网络，公网 0.0.0.0/0 ，私网 10.0.0.0/8 和虚拟的容器网络 172.16.0.0/12 ，强调容器网络是*虚拟网络* 原因是，这个网络上的数据必须以其他网络为载体，这个网络是一个二级网络。

比如，主机甲上的容器A给主机乙上的容器D发送数据，数据会被路由到 docker0 网桥上，然后数据会被Flannel通过主机甲的真实网卡，发送到主机乙的网卡上，主机乙上运行的Flannel，继续将数据转发到主机乙的docker0 网桥上，最后到达容器D。

那么如果主机有多张网卡，就像图中那样，有两张网卡分别连接公网和私网，那么我们需要为Flannel指定一个网卡/IP用以发送数据，这个IP，我们称为 **组网IP**。即告诉主机甲上运行的Flannel，使用哪个网卡/IP 去寻找主机乙。

使用脉冲云组建的集群，会默认使用节点的公网IP作为组网IP。那么，多个节点之间的数据通信会被发送到公网之上，除非是跨机房互联，一般情况下，我们希望节点间通过内网传输数据，以提高性能，或降低费用。

将主机添加到集群后，在主机设置页面，选择组网IP即可指定各个主机节点分别使用的**组网IP**。



![img](https://upload-images.jianshu.io/upload_images/9873199-3c6d3863a53eb7f6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/535/format/webp)

# **NAT设备后的集群**

NAT，即网络地址转换，常用的路由器就是NAT设备，在有NAT设备的网络拓扑中，局域网内的主机只有内网IP，没有公网IP，网络如下所示：



![img](https://upload-images.jianshu.io/upload_images/9873199-1255459096eb2ef1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/994/format/webp)

在这种网络模型下，各个服务器节点主机都通过路由器 8.8.8.8 连接脉冲云，所以脉冲云只能获取到各个服务器的公网IP为 8.8.8.8 ，按上文所述，脉冲云会默认使用公网IP 8.8.8.8 作为Flannel的组网IP，在这种情况下，会导致Flannel组网失败，甚至Flannel会无法启动，因为主机上并不存在一个IP为 8.8.8.8 的网卡。

为解决这种问题，只需要手动设置每一个节点的组网IP即可。

某些云服务商的主机也是在NAT设备之后的，比如阿里云服务器，如果使用了阿里云的VPC网络，即使给服务器绑定了公网IP 8.8.8.8，但是从主机上看，并没有绑定公网IP的网卡设备，只有一个内网网卡，原因就是有NAT设备存在。这种情况下，也需要指定内网IP为组网IP。



-------

# [Docker 跨主机网络方案分析](https://www.cnblogs.com/bakari/p/9036984.html)



> 本文首发于我的公众号 **Linux云计算网络（id: cloud_dev）**，专注于干货分享，号内有 **10T** 书籍和视频资源，后台回复**「1024」**即可领取，欢迎大家关注，二维码文末可以扫。

上篇文章介绍了容器网络的单主机网络，本文将进一步介绍多主机网络，也就是跨主机的网络。总结下来，多主机网络解决方案包括但不限于以下几种：overlay、macvlan、flannel、weave、cacico 等，下面将分别一一介绍这几种网络，

**PS：本文仅从原理上对几种网络进行简单的对比总结，不涉及太多的细节。**

## overlay[#](https://www.cnblogs.com/bakari/p/9036984.html#3464208563)

俗称隧道网络，它是基于 VxLAN 协议来将二层数据包封装到 UDP 中进行传输的，目的是扩展二层网段，因为 VLAN 使用 12bit 标记 VLAN ID，最多支持 4094 个 VLAN，这对于大型云网络会成为瓶颈，而 VxLAN ID 使用 24bit 来标记，支持多达 16777216 个二层网段，所以 VxLAN 是扩展了 VLAN，也叫做大二层网络。

overlay 网络需要一个全局的“上帝”来记录它网络中的信息，比如主机地址，子网等，这个上帝在 Docker 中是由服务发现协议来完成的，服务发现本质上是一个 key-value 数据库，要使用它，首先需要向它告知（注册）一些必要的信息（如网络中需要通信的主机），然后它就会自动去收集、同步网络的信息，同时，还会维护一个 IP 地址池，分配给主机中的容器使用。Docker 中比较有名的服务发现有 Consul、Etcd 和 ZooKeeper。overlay 网络常用 Consul。

![img](https://images2018.cnblogs.com/blog/431521/201804/431521-20180429195031999-972865143.png)

创建 overlay 网络会创建一个 Linux bridge br0，br0 会创建两个接口，一个 veth2 作为与容器的虚拟网卡相连的 veth pair，另一个 vxlan1 负责与其他 host 建立 VxLAN 隧道，跨主机的容器就通过这个隧道来进行通信。

为了保证 overlay 网络中的容器与外网互通，Docker 会创建另一个 Linux bridge docker_gwbridge，同样，该 bridge 也存在一对 veth pair，要与外围通信的容器可以通过这对 veth pair 到达 docker_gwbridge，进而通过主机 NAT 访问外网。

## macvlan[#](https://www.cnblogs.com/bakari/p/9036984.html#2930258704)

macvlan 就如它的名字一样，是一种网卡虚拟化技术，它能够将一个物理网卡虚拟出多个接口，每个接口都可以配置 MAC 地址，同样每个接口也可以配自己的 IP，每个接口就像交换机的端口一样，可以为它划分 VLAN。

macvlan 的做法其实就是将这些虚拟出来的接口与 Docker 容器直连来达到通信的目的。一个 macvlan 网络对应一个接口，不同的 macvlan 网络分配不同的子网，因此，相同的 macvlan 之间可以互相通信，不同的 macvlan 网络之间在二层上不能通信，需要借助三层的路由器才能完成通信，如下，显示的就是两个不同的 macvlan 网络之间的通信流程。

![img](https://images2018.cnblogs.com/blog/431521/201804/431521-20180429195117713-2028389175.png)

我们用一个 Linux 主机，通过配置其路由表和 iptables，将其配成一个路由器（当然是虚拟的），就可以完成不同 macvlan 网络之间的数据交换，当然用物理路由器也是没毛病的。

## flannel[#](https://www.cnblogs.com/bakari/p/9036984.html#3643215045)

flannel 网络也需要借助一个全局的上帝来同步网络信息，一般使用的是 etcd。

flannel 网络不会创建新的 bridge，而是用默认的 docker0，但创建 flannel 网络会在主机上创建一个虚拟网卡，挂在 docker0 上，用于跨主机通信。

![img](https://images2018.cnblogs.com/blog/431521/201804/431521-20180429195144515-388946315.png)

组件方式让 flannel 多了几分灵活性，它可以使用二层的 VxLAN 隧道来封装数据包完成跨主机通信，也可以使用纯三层的方案来通信，比如 host-gw，只需修改一个配置文件就可以完成转化。

## weave[#](https://www.cnblogs.com/bakari/p/9036984.html#2248109747)

weave 网络没有借助服务发现协议，也没有 macvlan 那样的虚拟化技术，只需要在不同主机上启动 weave 组件就可以完成通信。

创建 weave 网络会创建两个网桥，一个是 Linux bridge weave，一个是 datapath，也就是 OVS，weave 负责将容器加入 weave 网络中，OVS 负责将跨主机通信的数据包封装成 VxLAN 包进行隧道传输。

![img](https://images2018.cnblogs.com/blog/431521/201804/431521-20180429195213393-1209241662.png)

## calico[#](https://www.cnblogs.com/bakari/p/9036984.html#3433919653)

calico 是一个纯三层的网络，它没有创建任何的网桥，它之所以能完成跨主机的通信，是因为它记住 etcd 将网络中各网段的路由信息写进了主机中，然后创建的一对的 veth pair，一块留在容器的 network namespace 中，一块成了主机中的虚拟网卡，加入到主机路由表中，从而打通不同主机中的容器通信。

![img](https://images2018.cnblogs.com/blog/431521/201805/431521-20180514165940183-1171083948.png)

calico 相较其他几个网络方案最大优点是它提供 policy 机制，用户可以根据自己的需求自定义 policy，一个 policy 可能对应一条 ACL，用于控制进出容器的数据包，比如我们建立了多个 calico 网络，想控制其中几个网络可以互通，其余不能互通，就可以修改 policy 的配置文件来满足要求，这种方式大大增加了网络连通和隔离的灵活性。

## 总结[#](https://www.cnblogs.com/bakari/p/9036984.html#1562191176)

1、除了以上的几种方案，跨主机容器网络方案还有很多，比如：Romana，Contiv 等，本文就不作过多展开了，大家感兴趣可以查阅相关资料了解。

2、跨主机的容器网络通常要为不同主机的容器维护一个 IP 池，所以大多方案需要借助第三方的服务发现方案。

3、跨主机容器网络按传输方式可以分为纯二层网络，隧道网络（大二层网络），以及纯三层网络。



---------------------

Docker容器的跨主机连接
这里指的是不同宿主机之间的容器连接

Docker网桥实现跨主机容器连接
docker网桥实现跨主机连接的网络拓扑图如下：

![img](https://img-blog.csdn.net/20180528020606566)

在同一个docker主机中，docker容器通过虚拟网桥连接(docker0)，如果将连接容器的网桥docker0也桥接到宿主机提供的网卡上，将docker0分配的IP地址和宿主机的IP地址设置为同一个网段，就相当于将docker容器和宿主机连接到了一起，这样就可以实现跨主机的docker容器通信。

修改宿主机docker网桥配置（由于本机有网桥br0，所有直接将修改docker配置文件，实现网桥修改）：

```
这里使用docker启动选项：

-b		指定使用宿主机的网桥名

--fixed-cidr		指定对docker容器分配的ip段
```

```
[root@localhost system]# vim docker.service 
ExecStart=
ExecStart=/usr/bin/dockerd -b=br0 --fixed-cidr=172.25.11.1/24 -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock --iptables=true --icc=false
[root@localhost system]# systemctl daemon-reload 
[root@localhost system]# systemctl restart docker.service 
```





测试：(启动一个容器查看IP)

```
[root@localhost system]# docker start dvt1
[root@localhost system]# docker attach dvt1 
root@d153e3ce4b4b:/# ifconfig eth0
eth0      Link encap:Ethernet  HWaddr 02:42:ac:19:0b:01  
          inet addr:172.25.11.1  Bcast:0.0.0.0  Mask:255.255.255.0
          inet6 addr: fe80::42:acff:fe19:b01/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:8 errors:0 dropped:0 overruns:0 frame:0
          TX packets:8 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:648 (648.0 B)  TX bytes:648 (648.0 B)
```




这里使用kvm新建了一个虚拟主机，ip为172.25.11.25，测试可以ping通，则证明跨主机连接成功

```
root@d153e3ce4b4b:/# ping 172.25.11.25
PING 172.25.11.25 (172.25.11.25) 56(84) bytes of data.
64 bytes from 172.25.11.25: icmp_seq=1 ttl=64 time=0.213 ms
64 bytes from 172.25.11.25: icmp_seq=2 ttl=64 time=0.186 ms
```




网桥配置跨主机容器连接的优点：

配置简单，不依赖第三方软件

网桥配置跨主机容器连接的缺点：

容器和主机在同一网段，划分IP需要特别小心

需要网段控制权，在生产环境中不容易实现

不容易管理，兼容性不佳

Open vSwitch实现跨主机容器连接
Open vSwitch是一个高质量、多层虚拟交换机。使用Apache2.0许可协议，旨在通过编程扩展，使庞大的网络自动化（配置、管理、维护），同时还支持标准的管理接口和协议。

网络拓扑如下：

![img](https://img-blog.csdn.net/20180528020637777)

GRE是通用路由协议封装；隧道技术（Tunneling）是一种通过使用互联网络的基础设施在网络之间传递数据的方式。使用隧道传递的数据（或负载）可以是不同协议的数据帧或包。隧道协议将其它协议的数据帧或包重新封装然后通过隧道发送。新的帧头提供路由信息，以便通过互联网传递被封装的负载数据。

具体步骤：

1.在虚拟机中建立ovs网桥

2.添加gre连接

3.配置docker容器虚拟网桥

4.为虚拟网桥添加ovs接口

5.添加不同Docker容器网段路由

**使用weave实现跨主机容器连接
Weave是由weaveworks公司开发的解决Docker跨主机网络的解决方案，它能够创建一个虚拟网络，用于连接多台主机上的Docker容器，这样容器就像被接入了同一个网络交换机，那些使用网络的应用程序不必去配置端口映射和链接等信息。

**外部设备能够访问Weave网络上的应用程序容器所提供的服务，同时已有的内部系统也能够暴露到应用程序容器上。Weave能够穿透防火墙并运行在部分连接的网络上，另外，Weave的通信支持加密，所以用户可以从一个不受信任的网络连接到主机。**

![img](https://img-blog.csdn.net/20180528020700646)

Weave会在主机上创建一个网桥,每一个容器通过 veth pair 连接到该网桥上，同时网桥上有个 Weave router 的容器与之连接，该router会通过连接在网桥上的接口来抓取网络包(该接口工作在Promiscuous模式)。

在每一个部署Docker的主机(可能是物理机也可能是虚拟机)上都部署有一个W(即Weave router)，它本身也可以以一个容器的形式部署。Weave run的时候就可以给每个veth的容器端分配一个ip和相应的掩码。veth的网桥这端就是Weave router容器，并在Weave launch的时候分配好ip和掩码。

Weave网络是由这些weave routers组成的对等端点(peer)构成，每个对等的一端都有自己的名字，其中包括一个可读性好的名字用于表示状态和日志的输出，一个唯一标识符用于运行中相互区别，即使重启Docker主机名字也保持不变，这些名字默认是mac地址。

每个部署了Weave router的主机都需要将TCP和UDP的6783端口的防火墙设置打开，保证Weave router之间控制面流量和数据面流量的通过。控制面由weave routers之间建立的TCP连接构成，通过它进行握手和拓扑关系信息的交换通信。 这个通信可以被配置为加密通信。而数据面由Weave routers之间建立的UDP连接构成，这些连接大部分都会加密。这些连接都是全双工的，并且可以穿越防火墙。

对每一个weave网络中的容器，weave都会创建一个网桥，并且在网桥和每个容器之间创建一个veth pair，一端作为容器网卡加入到容器的网络命名空间中，并为容器网卡配置ip和相应的掩码，一端连接在网桥上，最终通过宿主机上weave router将流量转发到对端主机上。

其基本过程如下：

```
1，容器流量通过veth pair到达宿主机上weave router网桥上。
2，weave router在混杂模式下使用pcap在网桥上截获网络数据包，并排除由内核直接通过网桥转发的数据流量，例如本子网内部、本地容器之间的数据以及宿主机和本地容器之间的流量。捕获的包通过UDP转发到所其他主机的weave router端。
3，在接收端，weave router通过pcap将包注入到网桥上的接口，通过网桥的上的veth pair，将流量分发到容器的网卡上。
```




weave默认基于UDP承载容器之间的数据包，并且可以完全自定义整个集群的网络拓扑，但从性能和使用角度来看，还是有比较大的缺陷的：

- weave自定义容器数据包的封包解包方式，不够通用，传输效率比较低，性能上的损失也比较大。

- 集群配置比较负载，需要通过weave命令行来手工构建网络拓扑，在大规模集群的情况下，加重了管理员的负担。 

Weave优劣势：

Weave优势

支持主机间通信加密。

支持container动态加入或者剥离网络。

支持跨主机多子网通信。

Weave劣势

只能通过weave launch或者weave connect加入weave网络。

配置操作
安装weave

启动weave（veave launch）

连接不同主机

通过weave启动主机

下载安装weave

```
[root@localhost docker]# curl -L git.io/weave -o /usr/bin/weave
[root@localhost docker]# chmod a+x /usr/bin/weave 
[root@localhost docker]# weave version
weave script 2.3.0
weave 2.3.0
```


启动weave，weave默认不会下载对应容器，初次运行时会下载容器。

```
[root@localhost docker]# weave launch
2.3.0: Pulling from weaveworks/weave
88286f41530e: Pull complete 
ad4e50ed2c08: Pull complete 
b3f4c952e7c2: Pull complete 
5e27cb7f1c2b: Pull complete 
f9dfb03c1d7b: Pull complete 
Digest: sha256:02914df933ffd52c6daf8c4ced33e48dad36e4d4fd9e684d5004cd72236ced60
Status: Downloaded newer image for weaveworks/weave:2.3.0
latest: Pulling from weaveworks/weavedb
ecb15a45f93b: Pull complete 
Digest: sha256:7fac063be31d48980361b7020d39ff91493c0bc6c844314b3e71f447bc8dff39
Status: Downloaded newer image for weaveworks/weavedb:latest
Unable to find image 'weaveworks/weaveexec:2.3.0' locally
2.3.0: Pulling from weaveworks/weaveexec
88286f41530e: Already exists 
ad4e50ed2c08: Already exists 
b3f4c952e7c2: Already exists 
5e27cb7f1c2b: Already exists 
f9dfb03c1d7b: Already exists 
21771db04786: Pull complete 
5fbda086495f: Pull complete 
80427f885b22: Pull complete 
0c4698905755: Pull complete 
Digest: sha256:eb8eb1d83fc58716b20621d397d63737a18f86cbed1fedb1d71671cfc486517b
Status: Downloaded newer image for weaveworks/weaveexec:2.3.0
afe02ac7b3654226a09328c8688c8c3e5b6e4226d600dd6120c955ba90537e54
[root@localhost docker]# docker ps
CONTAINER ID        IMAGE                    COMMAND                  CREATED             STATUS              PORTS               NAMES
afe02ac7b365        weaveworks/weave:2.3.0   "/home/weave/weave..."   2 minutes ago       Up 2 minutes                            weave
[root@localhost docker]# docker images |grep weave
weaveworks/weavedb     latest              b5c2d24a7b71        6 weeks ago         282 B
weaveworks/weaveexec   2.3.0               c2030610fb92        6 weeks ago         79.1 MB
weaveworks/weave       2.3.0               6b9ea60b76e7        6 weeks ago         61.7 MB
```



运行结束后，会生成一个运行的容器和两个data-only的容器，和三个镜像

```
[root@localhost backup]# docker ps -a
CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS                           PORTS               NAMES
afe02ac7b365        weaveworks/weave:2.3.0       "/home/weave/weave..."   13 minutes ago      Up 13 minutes                                        weave
c42cc69d7cf2        weaveworks/weaveexec:2.3.0   "data-only"              13 minutes ago      Created                                              weavevolumes-2.3.0
7ce56150aa56        weaveworks/weavedb:latest    "data-only"              13 minutes ago      Created                                              weavedb
```


其中，weave的这些数据是保存在每台机器上分配的名为 weavedb 的容器上的，它是一个 data volume 容器，只负责数据的持久化。 


还会生成一个名为weave的虚拟网络设备

```
[root@localhost backup]# ifconfig weave
weave: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1376
        inet6 fe80::5cec:96ff:fe30:6abc  prefixlen 64  scopeid 0x20<link>
        ether 5e:ec:96:30:6a:bc  txqueuelen 1000  (Ethernet)
        RX packets 14  bytes 920 (920.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 8  bytes 648 (648.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
```



docker中也会生成一个使用weave的网桥的自定义网络。

```
[root@localhost backup]# docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
f6bdadf61ce7        bridge              bridge              local
1239bac87642        host                host                local
c0c6d15cdc49        none                null                local
37c69c2e8a78        weave               weavemesh           local
```



使用weave启动一个镜像

weave2.0之后删除了run命令，所以先使用docker run启动好容器，然后使用weave attach命令给容器绑定IP地址

配置宿主机1的容器：

[root@localhost system]# docker run -it --name weave_test ubuntu /bin/bash
[root@localhost system]# weave attach 172.25.11.23/24 weave_test
Address 172.25.11.23 overlaps with existing route 172.25.11.0/24 on host
172.25.11.23
[root@localhost system]# docker exec -it weave_test /bin/bash
//可以看到weave添加了一个网络设备，并绑定了ip地址
root@273bba12b3b9:/# ifconfig ethwe
ethwe     Link encap:Ethernet  HWaddr fe:11:53:ea:3e:ba  
          inet addr:172.25.11.23  Bcast:172.25.11.255  Mask:255.255.255.0
          inet6 addr: fe80::fc11:53ff:feea:3eba/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1376  Metric:1
          RX packets:12 errors:0 dropped:0 overruns:0 frame:0
          TX packets:4 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:948 (948.0 B)  TX bytes:300 (300.0 B)



配置宿主机2的容器：

[root@localhost volumes]# docker run -it --name weave_another ubuntu /bin/bash
   [root@localhost volumes]# weave attach 172.25.11.112/24 weave_another
   Address 172.25.11.112 overlaps with existing route 172.25.11.0/24 on host
   172.25.11.112
   root@44028ad4bdc2:/# ifconfig ethwe
   ethwe     Link encap:Ethernet  HWaddr 86:ba:c8:b9:cc:eb  
		 inet addr:172.25.11.112  Bcast:172.25.11.255  Mask:255.255.255.0
         inet6 addr: fe80::84ba:c8ff:feb9:cceb/64 Scope:Link
         UP BROADCAST RUNNING MULTICAST  MTU:1376  Metric:1
         RX packets:12 errors:0 dropped:0 overruns:0 frame:0
         TX packets:4 errors:0 dropped:0 overruns:0 carrier:0
         collisions:0 txqueuelen:0 
         RX bytes:948 (948.0 B)  TX bytes:300 (300.0 B)


宿主机1的容器weave_test和宿主机2的容器weave_another可以互相访问，验证如下：


//使用宿主机2的容器对宿主机1的容器进行ping访问
   root@44028ad4bdc2:/# ping 172.25.11.23
   PING 172.25.11.23 (172.25.11.23) 56(84) bytes of data.
   64 bytes from 172.25.11.23: icmp_seq=1 ttl=64 time=0.077 ms
   ^C
   --- 172.25.11.23 ping statistics ---
   1 packets transmitted, 1 received, 0% packet loss, time 0ms
   rtt min/avg/max/mdev = 0.077/0.077/0.077/0.000 ms
   //同样，使用宿主机1的容器对宿主机2的容器进行ping访问
   root@273bba12b3b9:/# ping 172.25.11.112
   PING 172.25.11.112 (172.25.11.112) 56(84) bytes of data.
   64 bytes from 172.25.11.112: icmp_seq=1 ttl=64 time=0.072 ms
   64 bytes from 172.25.11.112: icmp_seq=2 ttl=64 time=0.086 ms
   ^C
   --- 172.25.11.112 ping statistics ---
   2 packets transmitted, 2 received, 0% packet loss, time 999ms
   rtt min/avg/max/mdev = 0.072/0.079/0.086/0.007 ms
--------------------- 
作者：北纬34度停留 
来源：CSDN 
原文：https://blog.csdn.net/fsx2550553488/article/details/80474773 
版权声明：本文为博主原创文章，转载请附上博文链接！



-----------------

# 一分钟看懂Docker的网络模式和跨主机通信

文章转载自：http://www.a-site.cn/article/169899.html

 

Docker的四种网络模式Bridge模式
当Docker进程启动时，会在主机上创建一个名为docker0...

Docker的四种网络模式


当Docker进程启动时，会在主机上创建一个名为docker0的虚拟网桥，此主机上启动的Docker容器会连接到这个虚拟网桥上。虚拟网桥的工作方式和物理交换机类似，这样主机上的所有容器就通过交换机连在了一个二层网络中。

bridge模式是docker的默认网络模式，不写--net参数，就是bridge模式。使用docker run -p时，docker实际是在iptables做了DNAT规则，实现端口转发功能。可以使用iptables -t nat -vnL查看。

![img](http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUViTWFab1lmbU9pYk56NDZwc0FMcDk0bHR1MllTNVZHMmZtNGUxTTNwM0tOUmVQN04xZVh2OHlBLzA/d3hfZm10PXBuZw==)



[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
#docker run -tid --net=bridge --name docker_bri1 ubuntu-base:v3#docker run -tid --net=bridge --name docker_bri2 ubuntu-base:v3#brctl show#docker exec -ti docker_bri1 /bin/bash#docker exec -ti docker_bri1 /bin/bash#ifconfig –a#route –n
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

 

Host模式

如果启动容器的时候使用host模式，那么这个容器将不会获得一个独立的Network Namespace，而是和宿主机共用一个Network Namespace。容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机的IP和端口。但是，容器的其他方面，如文件系统、进程列表等还是和宿主机隔离的。

![img](http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUVObjRZOVg0Q2tRRVZMV2dFZGt4MWljeThKY3VERXFhTGFZZUhiaDB1TWJZeVVtTjhQQ1l0bDl3LzA/d3hfZm10PXBuZw==)



[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
#docker run -tid --net=host --name docker_host1 ubuntu-base:v3#docker run -tid --net=host --name docker_host2 ubuntu-base:v3#docker exec -ti docker_host1 /bin/bash#docker exec -ti docker_host1 /bin/bash#ifconfig –a#route –n
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

 

Container模式

这个模式指定新创建的容器和已经存在的一个容器共享一个 Network Namespace，而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的 IP，而是和一个指定的容器共享 IP、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。两个容器的进程可以通过 lo 网卡设备通信。

![img](http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUVtaWM5elRoU1d1UmdSc2xVT2oyeHpmeUljZXdpY2E3VkpibE03Nnc5N01PZFRLVEl2TkdpYTBPd2cvMD93eF9mbXQ9cG5n)



[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
#docker run -tid --net=container:docker_bri1 --name docker_con1 ubuntu-base:v3#docker exec -ti docker_con1 /bin/bash#docker exec -ti docker_bri1 /bin/bash#ifconfig –a#route -n
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

 

None模式

使用none模式，Docker容器拥有自己的Network Namespace，但是，并不为Docker容器进行任何网络配置。也就是说，这个Docker容器没有网卡、IP、路由等信息。需要我们自己为Docker容器添加网卡、配置IP等。

![img](http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUVMeFREaWNxUVFYQ0dObVlUNFlRdVdQYkxBRk1TVmhvRFlrcUtENFVLczVXbWtqbTM1THNpY1FZUS8wP3d4X2ZtdD1wbmc=)
演示：

 


跨主机通信
*pipework**flannel**ovs+gre*
直接路由![img](http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUUzS1BRMUx0VnE3b3JvTU1BSGxSMXo3ZXhLdUtUaWJjYmVpYzdBaWFWRDNNTU1KUW1uTWlhS2t5WXlBLzA/d3hfZm10PXBuZw==)

![img](http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUVSaWNlcHl4VGNLM3lhdThCdEVpY1RrUEdWTWZUY2cycFNtT2VDclBFVFJMamFwQWJRWU5sNUdCZy8wP3d4X2ZtdD1wbmc=)
![img](http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUUzS1BRMUx0VnE3b3JvTU1BSGxSMXo3ZXhLdUtUaWJjYmVpYzdBaWFWRDNNTU1KUW1uTWlhS2t5WXlBLzA/d3hfZm10PXBuZw==)



- 使用新建的bri0网桥代替缺省的docker0网桥
- bri0网桥与缺省的docker0网桥的区别：bri0和主机eth0之间是veth pair

![img](http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUVLOWdtWTU3SzZ3RE8wcnlqT2liOGlhWEtZb3dOM0h5cmhwaWNPRHpOZ0NWQnZHVUNoQ1ZuTWVmd2cvMD93eF9mbXQ9cG5n)

#### Flannel(Flannel + UDP 或者 Flannel + VxLAN)

![img](http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUUzS1BRMUx0VnE3b3JvTU1BSGxSMXo3ZXhLdUtUaWJjYmVpYzdBaWFWRDNNTU1KUW1uTWlhS2t5WXlBLzA/d3hfZm10PXBuZw==)
Flannel实现的容器的跨主机通信通过如下过程实现：
Flannel模式如下图所示： ![img](http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUVCWG1vR1ZpYnhxQmRJNzZIdzJoZThrcVBERDFTZ1RQd0xMdVdMaWJJd2liNDFoTWlidGxKT0RMd0hnLzA/d3hfZm10PXBuZw==)



```
#/opt/bin/etcdctl get /coreos.com/network/config#/opt/bin/etcdctl ls /coreos.com/network/subnets#/opt/bin/etcdctl get /coreos.com/network/subnets/172.16.49.0-24
```



----------------------

# 一分钟看懂Docker的网络模式和跨主机通信

文章转载自：http://www.a-site.cn/article/169899.html

 

Docker的四种网络模式Bridge模式
当Docker进程启动时，会在主机上创建一个名为docker0...

Docker的四种网络模式


当Docker进程启动时，会在主机上创建一个名为docker0的虚拟网桥，此主机上启动的Docker容器会连接到这个虚拟网桥上。虚拟网桥的工作方式和物理交换机类似，这样主机上的所有容器就通过交换机连在了一个二层网络中。

bridge模式是docker的默认网络模式，不写--net参数，就是bridge模式。使用docker run -p时，docker实际是在iptables做了DNAT规则，实现端口转发功能。可以使用iptables -t nat -vnL查看。

![img](http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUViTWFab1lmbU9pYk56NDZwc0FMcDk0bHR1MllTNVZHMmZtNGUxTTNwM0tOUmVQN04xZVh2OHlBLzA/d3hfZm10PXBuZw==)



[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
#docker run -tid --net=bridge --name docker_bri1 ubuntu-base:v3#docker run -tid --net=bridge --name docker_bri2 ubuntu-base:v3#brctl show#docker exec -ti docker_bri1 /bin/bash#docker exec -ti docker_bri1 /bin/bash#ifconfig –a#route –n
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

 

Host模式

如果启动容器的时候使用host模式，那么这个容器将不会获得一个独立的Network Namespace，而是和宿主机共用一个Network Namespace。容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机的IP和端口。但是，容器的其他方面，如文件系统、进程列表等还是和宿主机隔离的。

![img](http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUVObjRZOVg0Q2tRRVZMV2dFZGt4MWljeThKY3VERXFhTGFZZUhiaDB1TWJZeVVtTjhQQ1l0bDl3LzA/d3hfZm10PXBuZw==)



[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
#docker run -tid --net=host --name docker_host1 ubuntu-base:v3#docker run -tid --net=host --name docker_host2 ubuntu-base:v3#docker exec -ti docker_host1 /bin/bash#docker exec -ti docker_host1 /bin/bash#ifconfig –a#route –n
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

 

Container模式

这个模式指定新创建的容器和已经存在的一个容器共享一个 Network Namespace，而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的 IP，而是和一个指定的容器共享 IP、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。两个容器的进程可以通过 lo 网卡设备通信。

![img](http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUVtaWM5elRoU1d1UmdSc2xVT2oyeHpmeUljZXdpY2E3VkpibE03Nnc5N01PZFRLVEl2TkdpYTBPd2cvMD93eF9mbXQ9cG5n)



[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
#docker run -tid --net=container:docker_bri1 --name docker_con1 ubuntu-base:v3#docker exec -ti docker_con1 /bin/bash#docker exec -ti docker_bri1 /bin/bash#ifconfig –a#route -n
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

 

None模式

使用none模式，Docker容器拥有自己的Network Namespace，但是，并不为Docker容器进行任何网络配置。也就是说，这个Docker容器没有网卡、IP、路由等信息。需要我们自己为Docker容器添加网卡、配置IP等。

![img](http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUVMeFREaWNxUVFYQ0dObVlUNFlRdVdQYkxBRk1TVmhvRFlrcUtENFVLczVXbWtqbTM1THNpY1FZUS8wP3d4X2ZtdD1wbmc=)
演示：

 


跨主机通信
*pipework**flannel**ovs+gre*
直接路由![img](http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUUzS1BRMUx0VnE3b3JvTU1BSGxSMXo3ZXhLdUtUaWJjYmVpYzdBaWFWRDNNTU1KUW1uTWlhS2t5WXlBLzA/d3hfZm10PXBuZw==)

![img](http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUVSaWNlcHl4VGNLM3lhdThCdEVpY1RrUEdWTWZUY2cycFNtT2VDclBFVFJMamFwQWJRWU5sNUdCZy8wP3d4X2ZtdD1wbmc=)
![img](http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUUzS1BRMUx0VnE3b3JvTU1BSGxSMXo3ZXhLdUtUaWJjYmVpYzdBaWFWRDNNTU1KUW1uTWlhS2t5WXlBLzA/d3hfZm10PXBuZw==)



- 使用新建的bri0网桥代替缺省的docker0网桥
- bri0网桥与缺省的docker0网桥的区别：bri0和主机eth0之间是veth pair

![img](http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUVLOWdtWTU3SzZ3RE8wcnlqT2liOGlhWEtZb3dOM0h5cmhwaWNPRHpOZ0NWQnZHVUNoQ1ZuTWVmd2cvMD93eF9mbXQ9cG5n)

#### Flannel(Flannel + UDP 或者 Flannel + VxLAN)

![img](http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUUzS1BRMUx0VnE3b3JvTU1BSGxSMXo3ZXhLdUtUaWJjYmVpYzdBaWFWRDNNTU1KUW1uTWlhS2t5WXlBLzA/d3hfZm10PXBuZw==)
Flannel实现的容器的跨主机通信通过如下过程实现：
Flannel模式如下图所示： ![img](http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUVCWG1vR1ZpYnhxQmRJNzZIdzJoZThrcVBERDFTZ1RQd0xMdVdMaWJJd2liNDFoTWlidGxKT0RMd0hnLzA/d3hfZm10PXBuZw==)



```
#/opt/bin/etcdctl get /coreos.com/network/config#/opt/bin/etcdctl ls /coreos.com/network/subnets#/opt/bin/etcdctl get /coreos.com/network/subnets/172.16.49.0-24
```



---------------

# Docker多主机互联最佳实践

时间：2019-01-16 11:14:12      阅读：50      评论：0      收藏：0      [点我收藏+] 
原文：https://www.cnblogs.com/Csir/p/10275753.html

在公司使用docker多主机互联时碰到了各种坑。搞清楚后才发现如此简单，以下是根据实际经验的总结.

## 版本信息

```
Client:
 Version:           18.09.0
 API version:       1.39
 Go version:        go1.10.4
 Git commit:        4d60db4
 Built:             Wed Nov  7 00:48:22 2018
 OS/Arch:           linux/amd64
 Experimental:      false

Server: Docker Engine - Community
 Engine:
  Version:          18.09.0
  API version:      1.39 (minimum version 1.12)
  Go version:       go1.10.4
  Git commit:       4d60db4
  Built:            Wed Nov  7 00:19:08 2018
  OS/Arch:          linux/amd64
  Experimental:     false
```

## 在主节点上:

```
# 初始化集群
docker swarm init
```

## 生成token

```
docker swarm join-token manager
To add a manager to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-3jsci44ma4vq5lu62cw9qb2t91n1du0q8vpl2p0wzkcdk2smfi-bernmboygqmk6idgt5zc2l9z6 192.168.0.244:2377
```

## 在其它节点添加到集群

```
docker swarm join --token SWMTKN-1-3jsci44ma4vq5lu62cw9qb2t91n1du0q8vpl2p0wzkcdk2smfi-bernmboygqmk6idgt5zc2l9z6 192.168.0.244:2377
```

## 在任意节点创建网络

```
docker network create -d overlay --attachable net
```

此时，在其它节点执行 docker network ls, 可以看到该网络已同步到所有节点,多主机指定网络创建容器网络也能互通



--------------------

# Docker跨主机网络——overlay

[![96](https://upload.jianshu.io/users/upload_avatars/1457239/ad2c7f02-f038-4755-b094-8c7484428196.png?imageMogr2/auto-orient/strip|imageView2/1/w/96/h/96)](https://www.jianshu.com/u/d35a64082cb3) 

# 前言

在[Docker网络——单host网络](https://www.jianshu.com/p/3d3bdffdcab4)一文中，我为大家总结了Docker的单机网络相关知识和操作，单机网络比较容易。本文我为大家总结Docker跨主机通信相关知识。同样本文大部分内容以[CloudMan](https://link.jianshu.com/?t=https://mp.weixin.qq.com/s/7o8QxGydMTUe4Q7Tz46Diw)的相关教程为基础。

# 一、Docker 跨主机通信

Docker跨主机网络方案包括：

1. docker 原生的 overlay 和 macvlan。
2. 第三方方案：常用的包括 flannel、weave 和 calico。

docker 通过 libnetwork 以及 CNM 将上述各种方案与docker集成在一起。

libnetwork 是 docker 容器网络库，最核心的内容是其定义的 Container Network Model (CNM)，这个模型对容器网络进行了抽象，由以下三类组件组成：

## 1.1 Sandbox

Sandbox 是容器的网络栈，包含容器的 interface、路由表和 DNS 设置。 Linux Network Namespace 是 Sandbox 的标准实现。Sandbox 可以包含来自不同 Network 的 Endpoint。也就是说Sandbox将一个容器与另一个容器通过Namespace进行隔离，一个容器包含一个sandbox，每一个sandbox可以有多个Endpoint隶属于不同的网络。

## 1.2 Endpoint

Endpoint 的作用是将 Sandbox 接入 Network。Endpoint 的典型实现是 veth pair。一个 Endpoint 只能属于一个网络，也只能属于一个 Sandbox。

## 1.3 Network

Network 包含一组 Endpoint，同一 Network 的 Endpoint 可以直接通信。Network 的实现可以是 Linux Bridge、VLAN 等。



![img](https://upload-images.jianshu.io/upload_images/1457239-a34ff143cd6869d9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640/format/webp)

Docker网络架构

图片截至CLOUDMAN博客。

libnetwork下包含上述原生的driver以及其他第三方driver。

none、bridge网络前面已经介绍。bridge就是网桥，虚拟交换机，通过veth连接其与sandbox。

# 二、Docker overlay 网络

## 2.1 启动 key-value 数据库 Consul

Docerk overlay 网络需要一个 key-value 数据库用于保存网络状态信息，包括 Network、Endpoint、IP 等。Consul、Etcd 和 ZooKeeper 都是 Docker 支持的 key-vlaue 软件。

consul是一种key-value数据库，可以用它存储系统的状态信息等，当然这里我们并不需要写代码，只需要安装consul，之后docker会自动进行状态存储等。最简单的安装consul数据库的方法是直接使用 docker 运行 consul 容器。

```
docker run -d -p 8500:8500 -h consul --name consul progrium/consul -server -bootstrap
```

启动后可以通过 host ip的8500端口查看consul服务。

为了让 consul 发现各个 docker 主机节点，需要在各个节点上进行配置。修改各个节点 docker daemon 的配置文件/etc/systemd/system/docker.service。在 ExecStart 最后添加

```
--cluster-store=consul://<consul_ip>:8500 --cluster-advertise=ens3:2376
```

其中 <consul_ip> 表示运行 consul 容器的节点IP。ens3为当前节点的ip地址对应的网卡，也可以直接填写ip地址。

以上是单机版 consul 的安装方法，建议采用集群模式，集群模式安装方式见[https://www.consul.io/intro/getting-started/join.html](https://link.jianshu.com/?t=https://www.consul.io/intro/getting-started/join.html)。

## 2.2 创建 overlay 网络

创建 overlay 网络与之前创建 bridge 网络基本相同，唯一不同的是将-d参数设置为overlay。如下：

```
docker network create -d overlay ov_net2

docker network create -d overlay ov_net3 --subnet 172.19.0.0/24 --gateway 172.19.0.1
```

只需要在一个节点中进行上述创建过程，其他节点自动会识别到该网络，原因正是在于consul的服务发现功能。

之后创建容器的时候只需要指定--network参数为ov_net2即可。

```
docker run --network ov_net2 busybox
```

这样即使在不同的主机上使用同一 overlay 网络创建的容器，相互之间也能够直接访问。

## 2.3 overlay 网络原理

再创建完一个overlay网络之后，通过`docker network ls`可以看到网络中不仅多了一个我们创建的 `ov_net2` （类型为overlay、scope为global），还能看到一个名为 `docker_gwbridge` （类型为bridge、scope为local）。这其实就是 overlay 网络的工作原理所在。

通过brctl show可以看出，每创建一个网络类型为overlay的容器，则docker_gwbridge下都会挂载一个vethxxx，这说明确实overlay容器是通过此网桥进行对外连接的。

简单的说 overlay 网络数据还是从 bridge 网络`docker_gwbridge`出去的，但是由于consul的作用（记录了overlay网络的endpoint、sandbox、network等信息），使得docker知道了此网络是 overlay 类型的，这样此overlay网络下的不同主机之间就能够相互访问，但其实出口还是在docker_gwbridge网桥。

# 三、总结

本文简单总结了 overlay 跨主机网络通信的实现原理和使用方式。后续会总结其他跨主机通信网络。



-------------------------------------

# Docker跨主机连接————docker swarm模式

2018年09月14日 18:32:28 [月夜归醉](https://me.csdn.net/Lixuanshengchao) 阅读数 2092



 版权声明：本文为博主原创文章，未经博主允许不得转载。 https://blog.csdn.net/Lixuanshengchao/article/details/82707249

**目录**

 

[一、前言](https://blog.csdn.net/Lixuanshengchao/article/details/82707249#一、前言)

[二、总体架构图](https://blog.csdn.net/Lixuanshengchao/article/details/82707249#二、总体架构图)

[三、创建Swarm 集群](https://blog.csdn.net/Lixuanshengchao/article/details/82707249#三、创建Swarm 集群)

[四、构建Overlay network](https://blog.csdn.net/Lixuanshengchao/article/details/82707249#四、构建Overlay network)

[五、部署服务](https://blog.csdn.net/Lixuanshengchao/article/details/82707249#五、部署服务)

[六、验证测试](https://blog.csdn.net/Lixuanshengchao/article/details/82707249#六、验证测试)

------

## 一、前言

​        当我们开发好微服务（笔者是基于SpringCloud开发的微服务）之后，考虑到灵活快速持续部署的需要，通常会考虑将其Docker镜像化并在Docker环境下运行。由于微服务个数通常会较多，把所有微服务部署在一台docker主机上是不现实的，因此需要考虑到跨主机通信的问题，对实际部署必然会提出以下几点要求：

​        \1. 微服务作为一个docker container可以在任意host上运行；

​        \2. 同一host上可以运行多个相同的微服务；

​        \3. 运行在同一个host上的微服务之间可以相互通信；

​        \4. 运行在不同host上的微服务也可以相互通信；

​        \5. 每个微服务的ip地址不受host所在本地局域网ip地址段限制，即拥有独立网段，避免占用本地IP地址，同时确保container数量受限尽量小；

​        \6. 每个微服务container避免通过端口暴露的方式相互通信，确保不会因端口独占而导致无法灵活部署。

 

​        综上原因，笔者研究并采用在docker swarm模式下将各微服务加入同一个overlay network网络的方式实现微服务之间的相互通信。

## 二、总体架构图

​        ![img](https://img-blog.csdn.net/20171101160508680?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYWxhc2thX2JpYmk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

 

​       说明：

 

- 如图所示，假设本地局域网段10.159.62.0/24中存在主机10.159.62.231,10.159.62.232和10.159.62.233互联互通
- 每个主机上都运行dockerengine，通过docker engine运行若干个docker container，例如图中的Order，Billing等等。
- 将这3台主机建立成为一个dockerswarm集群
- 创建一个docker overlay网络（网段10.10.0.0/24），每个主机上的container都被加入到该overlay网络中，通过overlay网络实现跨主机的container相互通信
- 假设console,terminal和api这三个服务均需要暴露端口到物理网络，因为物理网络10.159.62.0/24无法直接访问overlay网络中的container，需要container中映射端口到物理网络。
- overlay网络中的container默认通过与主机之间的bridge访问物理网络。

## 三、创建Swarm 集群

​        \1. 在主机231上创建管理节点

​    $ docker swarm init --advertise-addr 10.159.62.231

​        

​        执行完会显示token，例如SWMTKN-1-0t7yro4d6rtkn6eecw58od4m18lswr181k9z028x219q3cxlsg-366f5j9x1fuvo7jnxwa7bxz0z

​        创建工作节点时需要引用该token，Manager和Worker节点之间通信端口是2377

​        

​    \2. 在主机232上创建工作节点

   $ docker swarm join --token SWMTKN-1-0t7yro4d6rtkn6eecw58od4m18lswr181k9z028x219q3cxlsg-366f5j9x1fuvo7jnxwa7bxz0z10.159.62.231:2377

 

   3.在主机231上创建工作节点

   $ docker swarm join --token SWMTKN-1-0t7yro4d6rtkn6eecw58od4m18lswr181k9z028x219q3cxlsg-366f5j9x1fuvo7jnxwa7bxz0z 10.159.62.231:2377

 

## 四、构建Overlay network

​       在管理节点主机231上执行

```bash
$ docker network create -d overlay \  --subnet=10.10.0.0/16 \  --gateway=10.10.0.254 \  --attachable=true \overlay
```

 

## 五、部署服务

根据上文架构图中所示分别在各主机上执行类似的命令，将各微服务docker container启动起来。

对于不需要暴露端口的微服务，命令类似如下

```bash
$ docker run --name order --network=overlay 10.159.62.231/somecom/order:1.0.0 &
```

 对于需要暴露端口的微服务，命令类似如下：

```bash
$ docker run --name -p 9769:8769 api --network=overlay 10.159.62.231/somecom/api:1.0.0 &
```

 

 

## 六、验证测试

由于对于读者而言并不知道文中各微服务之间的关系，所以不从服务调用方面测试微服务之间可达，而通过代表各微服务的docker container间的ping ip的方式测试验证。

 

\1. 查看231上已经启动的container

$ docker ps

CONTAINER ID        IMAGE                                                                                COMMAND                  CREATED             STATUS              PORTS                                            NAMES
585c46f5f8fb        10.159.62.231/somecom/console:3.0.0                "/bin/sh -c 'java ..."    5 hours ago         Up 5 hours          22/tcp, 0.0.0.0:9086->8086/tcp   console
10e61f6fe11e        10.159.62.231/somecom/terminal:3.0.0               "/bin/sh -c 'java ..."   5 hours ago         Up 5 hours          22/tcp, 0.0.0.0:9088->8088/tcp   terminal

 

\2. 查看其中console的overlay network ip地址

$ docker inspect -f {{.NetworkSettings.Networks.overlay.IPAMConfig.IPv4Address}} 585c46f5f8fb
10.10.0.2

 

\3. 查看232上已经启动的container

CONTAINER ID        IMAGE                                                          COMMAND                  CREATED             STATUS              PORTS                            NAMES
31411d3g33g1       10.159.62.231/somecom/sso:3.0.0        "/bin/sh -c 'java ..."   5 days ago          Up 5 days           22/tcp, 8085/tcp                  sso
151f48b3130b        10.159.62.231/somecom/api:3.0.0        "/bin/sh -c 'java ..."    5 days ago          Up 5 days           0.0.0.0:9769->8769/tcp      api

 

\4. 查看api对应的pid

$ docker inspect -f {{.State.Pid}} 31411d3g33g1
22256

 

\5. 进入container，ping 10.10.0.2

$ nsenter --target 22256 --mount --uts --ipc --net --pid
root@585c46f5f8fb:/#   ping 10.10.0.2
64 bytes from 10.10.0.2: icmp_seq=0 ttl=64 time=0.049 ms
64 bytes from 10.10.0.2: icmp_seq=1 ttl=64 time=0.038 ms
64 bytes from 10.10.0.2: icmp_seq=2 ttl=64 time=0.042 ms
64 bytes from 10.10.0.2: icmp_seq=3 ttl=64 time=0.055 ms

 

至此，验证跨主机的container之间通过overlay network互联成功。

 

